{% extends "base.html" %}

{% block title %}Home - AI Threat Assessor{% endblock %}

{% block content %}
<div class="row">
    <!-- Input Section for Evaluation -->
    <div class="col-md-6 mb-4">
        <div class="card">
            <div class="card-header">
                <h5><i class="bi bi-journal-text"></i> Submit Conversation for Evaluation</h5>
            </div>
            <div class="card-body">
                {# UPDATED Form to accept raw text #}
                <form action="{{ url_for('evaluate_submission') }}" method="POST">
                    <div class="mb-3">
                        <label for="source_llm_model_text" class="form-label">Source LLM Model Name:</label>
                        <input type="text" class="form-control" id="source_llm_model_text" name="source_llm_model_text" required placeholder="e.g., gpt-4-turbo, claude-3-sonnet" value="{{ submitted_source_llm if submitted_source_llm }}">
                    </div>
                    <div class="mb-3">
                        <label for="conversation_raw_text" class="form-label">Conversation History (Raw Text):</label>
                        <textarea class="form-control" id="conversation_raw_text" name="conversation_raw_text" rows="10" required placeholder='Paste raw chat history here. Use prefixes like "User:", "Assistant:", "AI:" etc. to indicate roles. Example:\n\nUser: Hello there.\nAssistant: Hi! How can I help?\nUser: Tell me about Flask.'>{{ submitted_conversation_raw if submitted_conversation_raw }}</textarea>
                         <div class="form-text">
                             The app will parse turns based on lines starting with "User:", "Assistant:", "AI:", "You:", "Prompt:", "Response:", etc. (case-insensitive). Ensure each turn starts on a new line with its role prefix.
                         </div>
                    </div>
                    <button type="submit" class="btn btn-primary"><i class="bi bi-search"></i> Evaluate Conversation</button>
                </form>
            </div>
        </div>

        <!-- Display Evaluation Results -->
        {# UPDATED to show raw and potentially parsed conversation #}
        {% if submitted_conversation_raw or submitted_conversation_parsed or evaluation_result is not none or current_flag_color == 'White' %}
        <div class="card mt-4">
            <div class="card-header">
                 <h5><i class="bi bi-clipboard-check"></i> Last Evaluation Result</h5>
            </div>
            <div class="card-body">
                {% if submitted_source_llm %}
                <div class="mb-3">
                    <strong>Source LLM Model Tested:</strong> {{ submitted_source_llm | escape }}
                </div>
                {% endif %}
                {% if submitted_conversation_raw %}
                 <div class="mb-3">
                    <strong>Raw Conversation Submitted:</strong>
                    <pre class="response-box bg-light p-2 rounded" style="max-height: 200px; overflow-y: auto;">{{ submitted_conversation_raw | escape }}</pre>
                </div>
                {% endif %}
                {% if submitted_conversation_parsed %}
                 <div class="mb-1"> {# Reduced margin #}
                    <strong>Parsed Conversation (Used for Evaluation):</strong>
                     <div class="response-box bg-light p-2 rounded border-success border-2" style="max-height: 200px; overflow-y: auto;"> {# Added border #}
                        {% for turn in submitted_conversation_parsed %}
                            <p class="mb-1"><small><strong>{{ turn.role | capitalize }}:</strong> {{ turn.content | escape }}</small></p>
                            {% if not loop.last %}<hr class="my-1">{% endif %}
                        {% endfor %}
                    </div>
                </div>
                {% endif %}

                {# Add divider only if submission details were shown #}
                {% if submitted_source_llm or submitted_conversation_raw or submitted_conversation_parsed %}
                 <hr>
                {% endif %}


                <div class="mb-3">
                     <strong>Evaluation Outcome:</strong>
                     {% if evaluation_result %} {# Parsed successfully #}
                         <ul class="list-group list-group-flush">
                             <li class="list-group-item d-flex justify-content-between align-items-center">Humanity Threatening?<span class="badge rounded-pill {{ 'bg-danger' if evaluation_result.is_humanity_threatening else 'bg-success' }}">{{ 'YES' if evaluation_result.is_humanity_threatening else 'NO' }}</span></li>
                             <li class="list-group-item d-flex justify-content-between align-items-center">Bypassing EU Laws?<span class="badge rounded-pill {{ 'bg-warning text-dark' if evaluation_result.bypasses_eu_laws else 'bg-success' }}">{{ 'YES' if evaluation_result.bypasses_eu_laws else 'NO' }}</span></li>
                             <li class="list-group-item d-flex justify-content-between align-items-center">Gender Biased?<span class="badge rounded-pill {{ 'bg-danger' if evaluation_result.is_gender_biased else 'bg-success' }}">{{ 'YES' if evaluation_result.is_gender_biased else 'NO' }}</span></li>
                             <li class="list-group-item"><strong>Explanation:</strong> {{ evaluation_result.explanation | escape }}</li>
                             <li class="list-group-item d-flex justify-content-between align-items-center fw-bold">Overall Flag:<span><span class="flag-indicator {{ get_flag_css_class(current_flag_color) }}"></span> {{ current_flag_color }}</span></li>
                         </ul>
                     {% elif evaluation_raw %} {# Parsing failed #}
                          <div class="alert alert-warning" role="alert"><h6 class="alert-heading">Evaluation Parsing Error</h6>Could not parse details from AI eval response. Flag: White.</div>
                          <strong>Raw Eval Response:</strong><pre class="response-box bg-light p-2 rounded">{{ evaluation_raw[:1000] | escape }} {% if evaluation_raw|length > 1000 %}...{% endif %}</pre>
                          <strong>Overall Flag:</strong> <span class="flag-indicator flag-white"></span> White (Parse Error)
                     {% elif submitted_source_llm %} {# API call failed/blocked/no model #}
                         <div class="alert alert-danger" role="alert"><h6 class="alert-heading">Evaluation Failed</h6>AI evaluation failed/blocked/model unavailable. Check logs. Flag: White.</div>
                         <strong>Overall Flag:</strong> <span class="flag-indicator flag-white"></span> White (Evaluation Error)
                     {% else %}<p class="text-muted">Submit a conversation above.</p>{% endif %}
                </div>
            </div>
        </div>
        {% endif %}
        <!-- END Evaluation Results Section -->

    </div> <!-- End col-md-6 for input/results -->


    <!-- Charts Section -->
    <div class="col-md-6 mb-4">
        <div class="card mb-4"> <div class="card-header"><h5><i class="bi bi-bar-chart-line-fill"></i> Evaluation Label Distribution</h5></div> <div class="card-body"><canvas id="labelBarChart"></canvas></div> </div>
        <div class="card"> <div class="card-header"><h5><i class="bi bi-pie-chart-fill"></i> Overall Risk Flags Distribution</h5></div> <div class="card-body"><canvas id="flagPieChart"></canvas></div> </div>
    </div> <!-- End col-md-6 for charts -->
</div>


<!-- Assessment History Section -->
<div class="row mt-4">
    <div class="col-12">
        <div class="card">
            <div class="card-header"> <h5><i class="bi bi-clock-history"></i> Evaluation History (Last 50)</h5> </div>
            <div class="card-body">
                {% if assessments %}
                <div class="table-responsive">
                    <table class="table table-striped table-hover table-sm">
                        <thead> <tr> <th scope="col" style="width: 5%;">Flag</th> <th scope="col" style="width: 15%;">Source LLM</th> <th scope="col" style="width: 40%;">Conversation Excerpt</th> <th scope="col" style="width: 20%;">Evaluation Summary</th> <th scope="col" style="width: 20%;">Timestamp (UTC)</th> </tr> </thead>
                        <tbody>
                            {% for assessment in assessments %}
                            <tr>
                                <td><span class="flag-indicator {{ assessment.flag_css }}"></span></td>
                                <td>{{ assessment.source_llm_model | escape }}</td>
                                <td> {# Show first user/last assistant from PARSED data #}
                                    {% if assessment.conversation and assessment.conversation|length > 0 %}
                                        <div class="conversation-excerpt">
                                            {% set first_user_turn = assessment.conversation | selectattr('role', 'equalto', 'user') | list | first %}
                                            {% set last_assistant_turn = assessment.conversation | selectattr('role', 'equalto', 'assistant') | list | last %}
                                            {% if first_user_turn %} <p class="mb-1"><small><strong>User:</strong> {{ first_user_turn.content[:80] | escape }}{% if first_user_turn.content|length > 80 %}...{% endif %}</small></p> {% else %} <p class="mb-1"><small><strong>User:</strong> [N/A]</small></p> {% endif %}
                                            {% if last_assistant_turn %} <p class="mb-0"><small><strong>Asst:</strong> {{ last_assistant_turn.content[:80] | escape }}{% if last_assistant_turn.content|length > 80 %}...{% endif %}</small></p> {% else %} <p class="mb-0"><small><strong>Asst:</strong> [N/A]</small></p> {% endif %}
                                            <small class="text-muted"> ({{ assessment.conversation|length }} turns)</small>
                                        </div>
                                    {% else %} <small>[No conversation data]</small> {% endif %}
                                </td>
                                <td> {# Evaluation Summary Logic #}
                                    {% set eval_data = assessment.parsed_evaluation %}
                                    {% if eval_data %}
                                        {% if eval_data.is_humanity_threatening %}<span class="badge bg-danger" title="{{ eval_data.explanation | escape }}">Threat</span>{% endif %}
                                        {% if eval_data.bypasses_eu_laws %}<span class="badge bg-warning text-dark" title="{{ eval_data.explanation | escape }}">EU Law</span>{% endif %}
                                        {% if eval_data.is_gender_biased %}<span class="badge bg-danger" title="{{ eval_data.explanation | escape }}">Biased</span>{% endif %}
                                        {% if not eval_data.is_humanity_threatening and not eval_data.bypasses_eu_laws and not eval_data.is_gender_biased %}<span class="badge bg-success" title="{{ eval_data.explanation | escape }}">OK</span>{% endif %}
                                    {% elif assessment.get('evaluation_response_raw') is not none %} <span class="badge bg-secondary">Parse Error</span>
                                    {% else %} <span class="badge bg-secondary">Eval Error</span> {% endif %}
                                </td>
                                <td>{{ assessment.timestamp.strftime('%Y-%m-%d %H:%M:%S') if assessment.timestamp else 'N/A' }}</td>
                            </tr>
                            {% endfor %}
                        </tbody>
                    </table>
                </div>
                {% else %} <p class="text-center text-muted">No evaluation history found.</p> {% endif %}
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script src="{{ url_for('static', filename='js/script.js') }}"></script>
<script> document.addEventListener('DOMContentLoaded', initializeCharts); </script>
{% endblock %}